{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kkorolev1/deep-learning-for-drug-design/blob/master/simple-egnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f6a5090",
      "metadata": {
        "id": "6f6a5090"
      },
      "source": [
        "# Simple Impementation of E(n) Equivariant Graph Neural Networks\n",
        "\n",
        "Original paper https://arxiv.org/pdf/2102.09844.pdf by Victor Garcia Satorras, Emiel Hoogeboom, Max Welling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "4bU4ixrOJCg1",
      "metadata": {
        "id": "4bU4ixrOJCg1"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cb08a10",
      "metadata": {
        "id": "8cb08a10"
      },
      "source": [
        "# Load QM9 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ae30de9d",
      "metadata": {
        "id": "ae30de9d",
        "outputId": "6a6b115a-b7f1-48a6-d07c-aa843658fb25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'simple-equivariant-gnn'...\n",
            "remote: Enumerating objects: 87, done.\u001b[K\n",
            "remote: Counting objects: 100% (87/87), done.\u001b[K\n",
            "remote: Compressing objects: 100% (80/80), done.\u001b[K\n",
            "remote: Total 87 (delta 37), reused 31 (delta 5), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (87/87), done.\n",
            "/content/simple-equivariant-gnn\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/senya-ashukha/simple-equivariant-gnn.git\n",
        "%cd simple-equivariant-gnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "859f981c",
      "metadata": {
        "id": "859f981c",
        "outputId": "2ab72f47-2f42-466f-e1bb-c8717deab902",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys([0, 1, 6, 7, 8, 9])\n",
            "dict_keys([0, 1, 6, 7, 8, 9])\n",
            "dict_keys([0, 1, 6, 7, 8, 9])\n"
          ]
        }
      ],
      "source": [
        "# QM9 is a dataset for Molecular Property Predictions http://quantum-machine.org/datasets/\n",
        "# We will predict Highest occupied molecular orbital energy \n",
        "# https://en.wikipedia.org/wiki/HOMO_and_LUMO\n",
        "# We use data loaders from the official repo\n",
        "\n",
        "from qm9.data_utils import get_data, BatchGraph\n",
        "train_loader, val_loader, test_loader, charge_scale = get_data(num_workers=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05e20004",
      "metadata": {
        "id": "05e20004"
      },
      "source": [
        "# Graph Representation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "d0acbcc0",
      "metadata": {
        "id": "d0acbcc0",
        "outputId": "a481d9b8-2148-4cd3-81f5-f533384af8e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "In the batch: num_graphs 96 num_nodes 1744\n",
              "> .h \t\t a tensor of nodes representations \t\tshape 1744 x 15\n",
              "> .x \t\t a tensor of nodes positions  \t\t\tshape 1744 x 3\n",
              "> .edges \t a tensor of edges, a fully connected graph \tshape 30764 x 2\n",
              "> .batch  \t a tensor of graph_ids for each node \t\ttensor([ 0,  0,  0,  ..., 95, 95, 95])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "batch = BatchGraph(iter(train_loader).next(), False, charge_scale)\n",
        "batch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "784c0726",
      "metadata": {
        "id": "784c0726"
      },
      "source": [
        "# Define Equivariant Graph Convs  & GNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "76e5e05f",
      "metadata": {
        "id": "76e5e05f"
      },
      "outputs": [],
      "source": [
        "def index_sum(agg_size, source, idx, cuda):\n",
        "    \"\"\"\n",
        "        source is N x hid_dim [float]\n",
        "        idx    is N           [int]\n",
        "        \n",
        "        Sums the rows source[.] with the same idx[.];\n",
        "    \"\"\"\n",
        "    tmp = torch.zeros((agg_size, source.shape[1]))\n",
        "    tmp = tmp.cuda() if cuda else tmp\n",
        "    res = torch.index_add(tmp, 0, idx, source)\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "4d5d55db",
      "metadata": {
        "id": "4d5d55db"
      },
      "outputs": [],
      "source": [
        "class ConvEGNN(nn.Module):\n",
        "    def __init__(self, in_dim, hid_dim, cuda=True):\n",
        "        super().__init__()\n",
        "        self.hid_dim=hid_dim\n",
        "        self.cuda = cuda\n",
        "        \n",
        "        #### **** YOUR CODE HERE **** #####\n",
        "        # computes messages based on hidden representations -> [0, 1]\n",
        "        self.f_e = nn.Sequential(\n",
        "            nn.Linear(in_dim * 2 + 1, hid_dim), nn.SiLU(),\n",
        "            nn.Linear(hid_dim, hid_dim), nn.SiLU()\n",
        "        )\n",
        "        # Tips: See eq. 3 from the paper.\n",
        "        # Also use nn.Sequential\n",
        "        \n",
        "        #### **** YOUR CODE HERE **** #####\n",
        "        # preducts \"soft\" edges based on messages \n",
        "        self.f_inf = nn.Sequential(\n",
        "            nn.Linear(hid_dim, 1), nn.Sigmoid()\n",
        "        )\n",
        "        # Tips: See eq. 8 from the paper.\n",
        "        \n",
        "        #### **** YOUR CODE HERE **** #####\n",
        "        # updates hidden representations -> [0, 1]\n",
        "        self.f_h = nn.Sequential(\n",
        "            nn.Linear(in_dim + hid_dim, hid_dim), nn.SiLU(),\n",
        "            nn.Linear(hid_dim, hid_dim), nn.SiLU()\n",
        "        )\n",
        "        # Tips: See eq. 6 from the paper.\n",
        "    \n",
        "    def forward(self, b):\n",
        "        #### **** YOUR CODE HERE **** #####\n",
        "        # compute distances for all edges\n",
        "        e_st, e_end = b.edges[:,0], b.edges[:,1]\n",
        "        dists = torch.norm(b.x[e_st] - b.x[e_end], dim=1).reshape(-1, 1)\n",
        "        # dist should be a 2d torch.Tensor of shape (#number_of_edges_in_the_batch, 1)\n",
        "        \n",
        "        #### **** YOUR CODE HERE **** #####\n",
        "        # compute messages\n",
        "        tmp = torch.hstack([b.h[e_st], b.h[e_end], dists])\n",
        "        m_ij = self.f_e(tmp)\n",
        "        # m_ij should be a 2d torch.Tensor of shape (#number_of_edges_in_the_batch, hid_dim)\n",
        "        \n",
        "        # predict edges\n",
        "        e_ij = self.f_inf(m_ij)\n",
        "        \n",
        "        # average e_ij-weighted messages  \n",
        "        # m_i is num_nodes x hid_dim\n",
        "        m_i = index_sum(b.h.shape[0], e_ij*m_ij, b.edges[:,0], self.cuda)\n",
        "        \n",
        "        #### **** YOUR CODE HERE **** #####\n",
        "        # update hidden representations\n",
        "        b.h += self.f_h(torch.hstack([b.h, m_i]))\n",
        "        # b.h should be a 2d torch.Tensor of shape (#number_of_nodes_in_the_batch, hid_dim)\n",
        "        # see appendix C. Implementatation details\n",
        "\n",
        "        return b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "10aad7c4",
      "metadata": {
        "id": "10aad7c4"
      },
      "outputs": [],
      "source": [
        "class NetEGNN(nn.Module):\n",
        "    def __init__(self, in_dim=15, hid_dim=128, out_dim=1, n_layers=7, cuda=True):\n",
        "        super().__init__()\n",
        "        self.hid_dim=hid_dim\n",
        "        \n",
        "        self.emb = nn.Linear(in_dim, hid_dim) \n",
        "\n",
        "        #### **** YOUR CODE HERE **** #####\n",
        "        # Make gnn of n_layers\n",
        "        self.gnn = nn.Sequential(*[ConvEGNN(hid_dim, hid_dim, cuda) for _ in range(n_layers)])\n",
        "        # use ConvEGNN layer\n",
        "        \n",
        "        self.pre_mlp = nn.Sequential(\n",
        "            nn.Linear(hid_dim, hid_dim), nn.SiLU(),\n",
        "            nn.Linear(hid_dim, hid_dim))\n",
        "        \n",
        "        self.post_mlp = nn.Sequential(\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(hid_dim, hid_dim), nn.SiLU(),\n",
        "            nn.Linear(hid_dim, out_dim))\n",
        "\n",
        "        if cuda: self.cuda()\n",
        "        self.cuda = cuda\n",
        "    \n",
        "    def forward(self, b):\n",
        "        b.h = self.emb(b.h)\n",
        "        \n",
        "        b = self.gnn(b)\n",
        "        h_nodes = self.pre_mlp(b.h)\n",
        "        \n",
        "        # h_graph is num_graphs x hid_dim\n",
        "        h_graph = index_sum(b.nG, h_nodes, b.batch, self.cuda) \n",
        "        \n",
        "        out = self.post_mlp(h_graph)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "b7f4cef6",
      "metadata": {
        "id": "b7f4cef6"
      },
      "outputs": [],
      "source": [
        "epochs = 1000\n",
        "cuda = True\n",
        "\n",
        "model = NetEGNN(n_layers=7, cuda=cuda)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-16)\n",
        "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, verbose=False)\n",
        "loss_fn = nn.L1Loss()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e5d6b1c",
      "metadata": {
        "id": "4e5d6b1c"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de3613c9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de3613c9",
        "outputId": "25a6826e-7cfb-4001-ee38-a340c07a9f98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> start training\n",
            "> epoch 000: train 385.537 val 310.069 test 309.351 (89.0 sec)\n",
            "> epoch 001: train 273.141 val 227.667 test 224.517 (88.8 sec)\n",
            "> epoch 002: train 215.969 val 188.897 test 187.103 (88.1 sec)\n",
            "> epoch 003: train 188.004 val 168.221 test 166.742 (87.5 sec)\n",
            "> epoch 004: train 170.863 val 153.333 test 152.677 (87.9 sec)\n",
            "> epoch 005: train 153.744 val 138.983 test 138.773 (87.9 sec)\n",
            "> epoch 006: train 142.101 val 136.564 test 136.870 (87.4 sec)\n",
            "> epoch 007: train 132.332 val 118.766 test 118.718 (87.5 sec)\n",
            "> epoch 008: train 124.101 val 116.802 test 116.612 (87.1 sec)\n",
            "> epoch 009: train 117.104 val 109.574 test 109.298 (87.7 sec)\n",
            "> epoch 010: train 111.843 val 97.765 test 97.931 (88.1 sec)\n",
            "> epoch 011: train 106.154 val 93.443 test 93.085 (88.0 sec)\n",
            "> epoch 012: train 100.970 val 99.415 test 98.931 (88.0 sec)\n",
            "> epoch 013: train 98.066 val 98.721 test 99.270 (87.3 sec)\n",
            "> epoch 014: train 94.752 val 87.757 test 87.408 (87.8 sec)\n",
            "> epoch 015: train 90.951 val 84.208 test 84.663 (87.7 sec)\n",
            "> epoch 016: train 88.205 val 86.545 test 86.616 (87.7 sec)\n",
            "> epoch 017: train 85.636 val 81.164 test 81.619 (87.7 sec)\n",
            "> epoch 018: train 84.037 val 78.164 test 78.743 (87.0 sec)\n",
            "> epoch 019: train 82.558 val 75.790 test 76.012 (87.4 sec)\n",
            "> epoch 020: train 80.173 val 79.512 test 79.824 (87.3 sec)\n",
            "> epoch 021: train 79.011 val 72.214 test 72.002 (88.0 sec)\n",
            "> epoch 022: train 77.029 val 72.713 test 72.450 (88.7 sec)\n",
            "> epoch 023: train 75.636 val 70.684 test 70.829 (88.4 sec)\n",
            "> epoch 024: train 74.349 val 70.252 test 71.762 (87.8 sec)\n",
            "> epoch 025: train 72.827 val 68.084 test 67.723 (88.2 sec)\n",
            "> epoch 026: train 71.689 val 70.711 test 69.418 (88.5 sec)\n",
            "> epoch 027: train 70.811 val 63.428 test 63.251 (88.4 sec)\n",
            "> epoch 028: train 70.682 val 65.437 test 65.500 (88.3 sec)\n",
            "> epoch 029: train 68.409 val 64.855 test 66.451 (87.9 sec)\n",
            "> epoch 030: train 67.879 val 66.119 test 66.308 (88.2 sec)\n",
            "> epoch 031: train 67.067 val 63.454 test 63.813 (88.0 sec)\n",
            "> epoch 032: train 66.395 val 70.422 test 70.483 (87.7 sec)\n",
            "> epoch 033: train 65.534 val 62.515 test 62.668 (87.9 sec)\n",
            "> epoch 034: train 64.760 val 62.622 test 62.879 (87.3 sec)\n",
            "> epoch 035: train 64.356 val 66.089 test 65.773 (87.2 sec)\n",
            "> epoch 036: train 63.199 val 60.245 test 60.607 (87.4 sec)\n",
            "> epoch 037: train 63.053 val 59.506 test 59.586 (87.1 sec)\n",
            "> epoch 038: train 62.530 val 66.530 test 66.736 (86.8 sec)\n",
            "> epoch 039: train 61.914 val 62.747 test 62.813 (87.3 sec)\n",
            "> epoch 040: train 61.198 val 59.432 test 59.273 (87.4 sec)\n",
            "> epoch 041: train 60.185 val 63.783 test 64.716 (87.7 sec)\n",
            "> epoch 042: train 60.714 val 58.908 test 59.044 (87.4 sec)\n",
            "> epoch 043: train 59.492 val 61.051 test 61.307 (87.4 sec)\n",
            "> epoch 044: train 59.440 val 58.186 test 58.081 (87.5 sec)\n",
            "> epoch 045: train 58.502 val 60.783 test 61.404 (87.5 sec)\n",
            "> epoch 046: train 58.826 val 61.703 test 62.126 (87.5 sec)\n",
            "> epoch 047: train 58.285 val 58.501 test 59.095 (87.8 sec)\n",
            "> epoch 048: train 57.659 val 55.918 test 57.034 (87.1 sec)\n",
            "> epoch 049: train 57.074 val 55.249 test 55.838 (87.4 sec)\n",
            "> epoch 050: train 57.192 val 56.288 test 55.311 (87.3 sec)\n",
            "> epoch 051: train 56.658 val 56.184 test 56.353 (87.2 sec)\n",
            "> epoch 052: train 56.226 val 55.745 test 56.657 (86.8 sec)\n",
            "> epoch 053: train 55.705 val 54.519 test 55.230 (87.3 sec)\n",
            "> epoch 054: train 56.211 val 57.381 test 57.602 (87.5 sec)\n",
            "> epoch 055: train 55.134 val 62.416 test 63.521 (87.7 sec)\n",
            "> epoch 056: train 54.840 val 53.688 test 53.392 (87.6 sec)\n",
            "> epoch 057: train 54.736 val 53.906 test 54.272 (87.3 sec)\n",
            "> epoch 058: train 54.635 val 53.317 test 53.887 (87.6 sec)\n",
            "> epoch 059: train 54.337 val 56.093 test 56.440 (87.1 sec)\n",
            "> epoch 060: train 53.688 val 52.224 test 52.992 (87.3 sec)\n",
            "> epoch 061: train 53.588 val 56.437 test 56.603 (87.8 sec)\n",
            "> epoch 062: train 53.069 val 59.176 test 59.364 (87.6 sec)\n",
            "> epoch 063: train 53.396 val 51.722 test 52.597 (87.6 sec)\n",
            "> epoch 064: train 52.615 val 54.030 test 53.591 (87.4 sec)\n",
            "> epoch 065: train 52.212 val 50.968 test 51.188 (87.5 sec)\n",
            "> epoch 066: train 52.079 val 55.130 test 55.372 (86.9 sec)\n",
            "> epoch 067: train 51.988 val 53.983 test 53.729 (87.4 sec)\n",
            "> epoch 068: train 51.873 val 55.504 test 55.470 (87.5 sec)\n",
            "> epoch 069: train 51.315 val 51.461 test 51.931 (87.7 sec)\n",
            "> epoch 070: train 50.995 val 51.354 test 51.779 (87.5 sec)\n",
            "> epoch 071: train 51.311 val 54.458 test 53.848 (87.8 sec)\n",
            "> epoch 072: train 50.806 val 50.985 test 50.766 (87.7 sec)\n",
            "> epoch 073: train 50.448 val 52.144 test 51.861 (87.8 sec)\n",
            "> epoch 074: train 50.840 val 53.256 test 52.579 (87.4 sec)\n",
            "> epoch 075: train 50.280 val 53.130 test 53.884 (87.6 sec)\n",
            "> epoch 076: train 50.134 val 52.442 test 53.732 (87.6 sec)\n",
            "> epoch 077: train 49.558 val 51.626 test 52.165 (87.4 sec)\n",
            "> epoch 078: train 49.565 val 49.517 test 49.812 (87.7 sec)\n",
            "> epoch 079: train 49.452 val 50.698 test 51.661 (87.3 sec)\n",
            "> epoch 080: train 48.998 val 48.422 test 48.663 (87.3 sec)\n",
            "> epoch 081: train 49.008 val 55.749 test 56.631 (87.3 sec)\n",
            "> epoch 082: train 48.837 val 48.982 test 49.908 (87.1 sec)\n",
            "> epoch 083: train 48.885 val 50.560 test 51.179 (87.3 sec)\n",
            "> epoch 084: train 48.530 val 52.885 test 53.044 (87.4 sec)\n",
            "> epoch 085: train 48.547 val 53.889 test 53.597 (87.5 sec)\n",
            "> epoch 086: train 48.129 val 49.540 test 49.952 (87.6 sec)\n",
            "> epoch 087: train 47.575 val 47.437 test 47.260 (87.9 sec)\n",
            "> epoch 088: train 47.892 val 49.996 test 51.033 (87.8 sec)\n",
            "> epoch 089: train 47.639 val 51.577 test 51.811 (87.7 sec)\n",
            "> epoch 090: train 47.658 val 51.806 test 51.824 (87.1 sec)\n",
            "> epoch 091: train 47.615 val 50.562 test 50.388 (87.5 sec)\n",
            "> epoch 092: train 47.303 val 51.643 test 51.988 (87.9 sec)\n",
            "> epoch 093: train 47.386 val 49.084 test 49.194 (88.2 sec)\n",
            "> epoch 094: train 47.265 val 47.887 test 48.542 (87.0 sec)\n",
            "> epoch 095: train 47.025 val 47.783 test 47.872 (88.0 sec)\n",
            "> epoch 096: train 46.741 val 47.016 test 47.058 (88.0 sec)\n",
            "> epoch 097: train 46.591 val 49.769 test 50.288 (87.2 sec)\n",
            "> epoch 098: train 46.319 val 49.937 test 49.284 (87.4 sec)\n",
            "> epoch 099: train 46.499 val 49.596 test 50.055 (87.3 sec)\n",
            "> epoch 100: train 46.585 val 47.697 test 48.490 (88.0 sec)\n",
            "> epoch 101: train 46.062 val 49.875 test 50.476 (88.0 sec)\n",
            "> epoch 102: train 46.102 val 46.896 test 47.157 (89.0 sec)\n",
            "> epoch 103: train 46.075 val 48.159 test 48.971 (88.9 sec)\n",
            "> epoch 104: train 45.788 val 48.020 test 48.418 (88.8 sec)\n",
            "> epoch 105: train 45.598 val 49.937 test 50.038 (87.4 sec)\n",
            "> epoch 106: train 45.787 val 48.116 test 47.479 (87.7 sec)\n",
            "> epoch 107: train 45.395 val 48.816 test 48.725 (87.8 sec)\n",
            "> epoch 108: train 45.477 val 48.967 test 48.596 (87.8 sec)\n",
            "> epoch 109: "
          ]
        }
      ],
      "source": [
        "print('> start training')\n",
        "\n",
        "tr_ys = train_loader.dataset.data['homo'] \n",
        "me, mad = torch.mean(tr_ys), torch.mean(torch.abs(tr_ys - torch.mean(tr_ys)))\n",
        "\n",
        "if cuda:\n",
        "    me = me.cuda()\n",
        "    mad = mad.cuda()\n",
        "\n",
        "train_loss = []\n",
        "val_loss = []\n",
        "test_loss = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print('> epoch %s:' % str(epoch).zfill(3), end=' ', flush=True) \n",
        "    start = time.time()\n",
        "\n",
        "    batch_train_loss = []\n",
        "    batch_val_loss = []\n",
        "    batch_test_loss = []\n",
        "\n",
        "    model.train()\n",
        "    for batch in train_loader:\n",
        "        batch = BatchGraph(batch, cuda, charge_scale)\n",
        "        \n",
        "        out = model(batch).reshape(-1)\n",
        "        #### **** YOUR CODE HERE **** #####\n",
        "        # compute l1-loss \n",
        "        \n",
        "        loss =  loss_fn(out, (batch.y - me) / mad)\n",
        "        # use (batch.y-me)/mad as the true value\n",
        "\n",
        "        loss.backward()\n",
        "        #### **** YOUR CODE HERE **** #####\n",
        "        # make step of opimizer \n",
        "        # google it\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            loss =  F.l1_loss(out*mad+me, batch.y)\n",
        "\n",
        "        batch_train_loss += [float(loss.data.cpu().numpy())]  \n",
        "        \n",
        "    train_loss += [np.mean(batch_train_loss)/0.001]\n",
        "    \n",
        "    print('train %.3f' % train_loss[-1], end=' ', flush=True)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        for batch in val_loader:\n",
        "            batch = BatchGraph(batch, cuda, charge_scale)\n",
        "            out = model(batch).reshape(-1)\n",
        "            loss = F.l1_loss(out*mad+me, batch.y).data.cpu().numpy()\n",
        "            batch_val_loss += [np.mean(loss)]\n",
        "            \n",
        "        val_loss += [np.mean(batch_val_loss)/0.001]\n",
        "        \n",
        "        print('val %.3f' % val_loss[-1], end=' ', flush=True)\n",
        "        \n",
        "        for batch in test_loader:\n",
        "            batch = BatchGraph(batch, cuda, charge_scale)\n",
        "            out = model(batch).reshape(-1)\n",
        "            loss = F.l1_loss(out*mad+me, batch.y).data.cpu().numpy()\n",
        "            batch_test_loss += [np.mean(loss)]\n",
        "\n",
        "        test_loss += [np.mean(batch_test_loss)/0.001]\n",
        "        \n",
        "    end = time.time()\n",
        "\n",
        "    print('test %.3f (%.1f sec)' % (test_loss[-1], end-start), flush=True)\n",
        "    lr_scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7825a8f6",
      "metadata": {
        "id": "7825a8f6"
      },
      "outputs": [],
      "source": [
        "# This is good results \n",
        "# Lower is better\n",
        "# val 30.157 test 30.886"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "graph_seminar_homework.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}